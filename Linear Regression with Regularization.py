# Advanced Certification Program in Computational Data Science A program by IISc
Mini-Project: Linear Regression with Regularization

Problem Statement

Predict the bike-sharing counts per hour based on the features including weather, day, time, humidity, wind speed, season e.t.c.

#Objectives

#Mini-Project scope:

      * perform data exploration and visualization
      * implement linear regression using sklearn and optimization
      * apply regularization on regression using Lasso, Ridge and Elasticnet techniques
      * calculate and compare the MSE value of each regression technique
      * analyze the features that are best contributing to the target

#Dataset

'''The dataset chosen for this mini-project is [Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset).
This dataset contains the hourly and daily count of rental bikes between the years 2011 and 2012 in the capital bike share system with the corresponding weather and 
seasonal information. This dataset consists of 17389 instances of each 16 features. 

Bike sharing systems are a new generation of traditional bike rentals where the whole process from membership, rental and return has become automatic. 
Through these systems, the user can easily rent a bike from a particular position and return to another position. Currently, there are about over 500 bike-sharing programs 
around the world which is composed of over 500 thousand bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental 
and health issues.

Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. 
As opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position are explicitly recorded in these systems. 
This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city.
Hence, it is expected that the most important events in the city could be detected via monitoring these data.'''


#Data Fields

          * dteday - hourly date
          * season - 1 = spring, 2 = summer, 3 = fall, 4 = winter
          * hr - hour
          * holiday - whether the day is considered a holiday
          * workingday - whether the day is neither a weekend nor holiday
          * weathersit -<br>
              1 - Clear, Few clouds, Partly cloudy, Partly cloudy <br>
              2 - Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist<br>
              3 - Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds<br>
              4 - Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog<br>   
          * temp - temperature in Celsius
          * atemp - "feels like" temperature in Celsius
          * humidity - relative humidity
          * windspeed - wind speed
          * casual - number of non-registered user rentals initiated
          * registered - number of registered user rentals initiated
          * cnt - number of total rentals

## Information

**Regularization:** It is a form of regression that shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, to avoid the risk of overfitting. A simple relation for linear regression looks like this.

$Y ≈ β_0 + β_1 X_1 + β_2 X_2 + …+ β_p X_p$

 Here $Y$ represents the learned relation and $β$ represents the coefficient estimates for different variables or predictors(X).
 
 If there is noise in the training data, then the estimated coefficients won’t generalize well to the future data. This is where regularization comes in and shrinks or regularizes these learned estimates towards zero.
 
Below are the Regularization techniques:
 
 * Ridge Regression
 * Lasso Regression
 * Elasticnet Regression

# Loading the Required Packages
import pandas as pd
import numpy as np
from sklearn import linear_model
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import r2_score

# Read the hour.csv file
df_hour = pd.read_csv("/content/hour.csv")

#print the first five rows of dataset

df_hour.head(5)

df_hour.shape

#print the datatypes of the columns

df_hour.dtypes

"#Task flow with respect to feature processing and model training

            * Explore and analyze the data

            * Identify continuous features and categorical features

            * Apply scaling on continuous features and one-hot encoding on categorical features

            * Separate the features, targets and split the data into train and test

            * Find the coefficients of the features using normal equation and find the cost (error)

            * Apply batch gradient descent technique and find the best coefficients

            * Apply SGD Regressor using sklearn

            * Apply linear regression using sklearn

            * Apply Lasso, Ridge, Elasticnet Regression

#Visualize the hour (hr) column with an appropriate plot and find the busy hours of bike sharing

#ploting hour vs number of bikes rented
fig = plt.figure(figsize=(10,5))
plt.bar(df_hour['hr'],df_hour['cnt'])
plt.xlabel("Hour")
plt.ylabel("Number of bikes rented")
plt.title("Bikes Rental Distribution"); #busy hours 7-8 am, 4-7 pm

#Visualize the distribution of count, casual and registered variables

#plotting count
fig = plt.figure(figsize=(21,8))
sns.histplot(data=df_hour, x='cnt', bins=100, kde=True);
plt.xlabel('Bikes Rented')
plt.ylabel('Frequency');

#plotting Casual
fig = plt.figure(figsize=(21,8))
sns.histplot(data=df_hour, x='casual', kde=True);
plt.xlabel('casual')
plt.ylabel('Frequency');

#plotting Registered
fig = plt.figure(figsize=(21,8))
sns.histplot(data=df_hour, x='registered', kde=True);
plt.xlabel('registered')
plt.ylabel('Frequency');

#Describe the relation of weekday, holiday and working day

#checking for null values
df_hour.isnull().sum()

#Number of bikes rented in weekdays
fig = plt.figure(figsize=(15,6))
sns.barplot(x='weekday', y = 'cnt', data=df_hour, estimator=sum)

# Holiday vs Count distribution
fig = plt.figure(figsize=(15,6))
sns.countplot(data=df_hour, x='holiday');

#Workingday Vs Count distribution
fig = plt.figure(figsize=(15,6))
sns.countplot(data=df_hour, x='workingday')

#Visualize the month wise count of both casual and registered for the year 2011 and 2012 separately.

df_hour.head(5)

df_stacked = pd.DataFrame(data=df_hour, columns=['yr','mnth','casual', 'registered'])
df_stacked

#Monthwise casual and registered count in 2011
df_stacked_11=df_stacked[df_stacked['yr']==0]
df_stacked_11=df_stacked_11.drop('yr', axis=1)
df_11_final = df_stacked_11.groupby(by='mnth', axis=0).sum().reset_index(drop=False)
df_11_final.plot(x='mnth', kind='bar', stacked = True, figsize=(15,8));

#Monthwise casuak and registered count in 2012
df_stacked_12=df_stacked[df_stacked['yr']==1]
df_stacked_12 = df_stacked_12.drop('yr', axis=1)
df_12_final = df_stacked_12.groupby(by='mnth', axis=0).sum().reset_index(drop=False)
df_12_final.plot(x='mnth', kind='bar', stacked = True, figsize=(15,8));

df_hour.head(5)

datatype= { "season": 'category', 'yr': 'category', 'hr':'category','mnth': 'category', 'workingday':'category','holiday': 'category', 'weekday': 'category', 'weathersit': 'category'}

#changing the data type to category
df_hour = df_hour.astype(datatype)

df_hour.info()

df_hour.describe()

#Analyze the correlation between features with heatmap

cor_matrix = df_hour.corr()

plt.figure(figsize=(15,8))
sns.heatmap(cor_matrix, annot=True);

#Checking outliers in casual
plt.figure(figsize=(15,8))
plt.boxplot(x='casual', data=df_hour);

#Checking outliers in registered
plt.figure(figsize=(15,8))
plt.boxplot(x='registered', data=df_hour);

#Pre-processing and Data Engineering
#droping the columns that are less important to the target based on the heatmap
data = df_hour.drop(['instant','windspeed'], axis=1)

data.head(5)

#Identify categorical and continuous variable

cat_data = data.select_dtypes(include='category')

cat_columns=cat_data.columns

cat_data.head(5)

con_data = data.select_dtypes(include=['float64', 'int'])

con_columns = con_data.columns

con_data.head(5)

#Feature scaling
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(con_data)

scaled_data = pd.DataFrame(scaled_data, columns=con_columns)

scaled_data.head(5)

#applying onehotencoder for categorical vairables
encoder = OneHotEncoder(sparse=False)
encoded_data = encoder.fit_transform(cat_data)
column_name = encoder.get_feature_names(cat_columns)
encoded_df = pd.DataFrame(encoded_data, columns=column_name)

encoded_df.head(5)

final_df= pd.concat([scaled_data,encoded_df], axis=1)

target = final_df[['casual', 'registered','cnt']]
features = final_df.drop(['casual', 'registered','cnt'], axis=1)

features.head(5)

#Implement the linear regression by finding the coefficients using below approaches

target_cnt = target['cnt']

x_train, x_test, y_train, y_test = train_test_split(features, target_cnt,test_size = .25, random_state = 100)

#Implementation using Normal Equation

from scipy.linalg import lstsq
x,res, rank, s = lstsq(x_train, y_train)

print(x)

#Implementing Linear regression using batch gradient descent

def error_fn(x, y, b):
 m = len(y)
 error = np.sum((x.dot(b)-y) ** 2)/(2 * m)
 return error

def batch_gradient_descent(x, y, b, alpha, iterations):
 error_value = [0] * iterations
 m = len(y)
 
 for iteration in range(iterations):
  h = x.dot(b)
  # Difference b/w Hypothesis and Actual Y
  loss = h-y
  # Gradient Calculation
  gradient = x.T.dot(loss) / m
  # Changing Values of B using Gradient
  b = b - alpha * gradient
  # New Cost Value
  error = error_fn(x, y, b)
  error_value[iteration] = error
 
 return b, error

b = np.zeros(x_train.shape[1])
alpha = 0.07
iter_ = 2000
new_b, error= batch_gradient_descent(x_train, y_train, b, alpha, iter_)

y_pred_gd = x_test.dot(new_b)

r2_gd = r2_score(y_test, y_pred_gd)
r2_gd

#SGD Regressor

from sklearn.linear_model import SGDRegressor
reg = SGDRegressor()
model = reg.fit(x_train,y_train)
predictions = model.predict(x_test)

rmse = mean_squared_error(y_test, predictions, squared=False)
print(rmse)

r2_sgd = r2_score(y_test, predictions)
r2_sgd

#Linear regression using sklearn

from sklearn.linear_model import LinearRegression
linreg = LinearRegression()
model_lin = linreg.fit(x_train, y_train)
predictions_lin = model_lin.predict(x_test)

rmse_lin = mean_squared_error(y_test, predictions_lin, squared=False)
print(rmse_lin)

#Calculate the $R^2$ (coefficient of determination) of the actual and predicted data

r_sqrd = r2_score(y_test, predictions_lin)
print(r_sqrd)

#Summarize the importance of features

lin_coeff = np.transpose(model_lin.coef_)

features_col= features.columns

features_col.shape

coeff_df = pd.DataFrame(lin_coeff, index=features_col)

coeff_df.head(5)

plt.figure(figsize=(20,8))
sns.barplot(x=coeff_df.index, y=coeff_df[0]);
plt.xticks(rotation=90);

#Regularization methods 
#Apply Lasso regression

#setting up alpha
alpha = [0.0001, 0.001,0.01, 0.1, 1, 10, 100]

rmse_las=[]
for i in alpha:
  las = linear_model.Lasso(alpha=i)
  model_las = las.fit(x_train,y_train)
  pred_las = model_las.predict(x_test)
  rmse_las.append(mean_squared_error(y_test, pred_las, squared=False))
  r2_las=r2_score(y_test, pred_las)
  print(r2_las)
print(rmse_las)

alpha_las_opt = alpha[rmse_las.index(min(rmse_las))]
alpha_las_opt

min(rmse_las)

las = linear_model.Lasso(alpha=alpha_las_opt)
model_las = las.fit(x_train,y_train)
pred_las = model_las.predict(x_test)

r2_las = r2_score(y_test, pred_las)
r2_las

#Apply Ridge regression

rmse_rid=[]
for i in alpha:
  rid = linear_model.Ridge(alpha=i)
  model_rid = rid.fit(x_train,y_train)
  pred_rid = model_rid.predict(x_test)
  rmse_rid.append(mean_squared_error(y_test, pred_rid, squared=False))

alpha_rid_opt = alpha[rmse_rid.index(min(rmse_rid))]
alpha_rid_opt

min(rmse_rid)

r2_rid = r2_score(y_test, pred_rid)
r2_rid

#Apply Elasticnet regression

rmse_en=[]
for i in alpha:
  en = linear_model.ElasticNet(alpha=i, l1_ratio = 0.5)
  model_en = en.fit(x_train,y_train)
  pred_en = model_en.predict(x_test)
  rmse_en.append(mean_squared_error(y_test, pred_en, squared=False))

alpha_en_opt = alpha[rmse_en.index(min(rmse_en))]
alpha_en_opt

min(rmse_en)

en = linear_model.ElasticNet(alpha=alpha_en_opt, l1_ratio = 0.5)
model_en = en.fit(x_train,y_train)
pred_en = model_en.predict(x_test)

r2_en = r2_score(y_test, pred_en)
r2_en

#Determine if there is a reduction in error if two target variables are considered

target_cas = target[['casual']]
target_reg = target[['registered']]

x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(features, target_cas,test_size = .25, random_state = 100)
x_train_3, x_test_3, y_train_3, y_test_3 = train_test_split(features, target_reg,test_size = .25, random_state = 100)

model_lin2 = linreg.fit(x_train_2, y_train_2)
pred_lin2 = model_lin2.predict(x_test_2)

model_lin3 = linreg.fit(x_train_3, y_train_3)
pred_lin3= model_lin3.predict(x_test_3)

rmse_lin2=mean_squared_error(y_test_2,pred_lin2, squared=False)
rmse_lin2

rmse_lin3=mean_squared_error(y_test_3,pred_lin3, squared=False)
rmse_lin3

r2_lin2 = r2_score(y_test_2, pred_lin2)
r2_lin2

r2_lin3 = r2_score(y_test_3, pred_lin3)
r2_lin3
